<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="initial-scale=1.0, minimum-scale=1.0, width=device-width">
    <style>
        .image-container {
            display: flex;
        }

        .image-box {
            width: 600px;
            height: 500px;
            margin-right: 10px;
        }

        .image-box img {
            width: 100%;
            height: 100%;
            object-fit: contain;
        }

        .image-box2 {
            width: 900px;
            height: 500px;
            margin-right: 10px;
        }

        .image-box2 img {
            width: 100%;
            height: 100%;
            object-fit: contain;
        }
    </style>
    <title>CHATS</title>
    <meta name="description" content="">
    <meta name="author" content="AnonResearch81">
    <script src="https://unpkg.com/wavesurfer.js@7"></script>
</head>

<body>
    <header>
        <h1>Towards human-like spoken dialogue generation between AI agents from written dialogue</h1>
        <p>
            submitted to ICLR 2024<br>
            Anonymous authors<br>
            <br>
            The advent of large language models (LLMs) has made it possible to generate natural written dialogues
            between two agents.
            However, generating human-like spoken dialogues from these written dialogues remains challenging.
            Spoken dialogues have several unique characteristics: they frequently include backchannels and laughter, and
            the smoothness of turn-takings significantly influences the fluidity of conversation.
            In this study, we present <i>CHATS</i> ― <b>CH</b>atty <b>A</b>gents
            <b>T</b>ext-to-<b>S</b>peech ― a discrete token-based system designed to generate spoken dialogues
            based on written dialogues.
            Our system can generate speech for both the speaker side and the listener side simultaneously, using only
            the transcription from the speaker side, which eliminates the need for transcriptions of backchannels or
            laughter.
            Moreover, CHATS facilitates natural turn-taking; it determines the appropriate duration of silence after
            each utterance when no overlap is present and, when overlap does occur, it initiates the generation of
            overlapping speech based on the phoneme sequence of the next utterance.
            Experimental evaluations indicate that CHATS outperforms standard text-to-speech baseline, producing spoken
            dialogues that are not only more interactive and fluid but also retain clarity and intelligibility.
        </p>
        <div class="image-container">
            <div class="image-box2">
                <img src="diagram.png">
            </div>
            <div class="image-box">
                <img src="uLM_architecture.png">
            </div>
        </div>
    </header>

    <h2>1. Test-set samples</h2>
    We extracted written dialogues consisting of 10 turns each from the test set and generated the corresponding spoken
    dialogue segments using the proposed system.
    Below, we present two samples of the generated audio, along with the corresponding written dialogues (in Japanese)
    and their automatic translations (in English).
    For comparison, we also present the recorded audio (Ground Truth), resynthesized audio (Resynthesized), and audio
    files generated by the dialogue generative spoken language model (dGSLM) [Nguyen et al., 2023], as well as a
    baseline system that operates two text-to-speech models in an alternating manner (Baseline).
    For dGSLM, we used 30 s segments of the recorded speech preceding these dialogues as prompts, and generated
    30 s continuations for each one.


    <h3>Sample 1</h3>
    <h4>Input written dialogue</h4>
    <table border="1" class="inlineTable" id="TextTable1">
        <col width="750">
        <col width="750">

        <tr>
            <td>Original (Japanese)</td>
            <td>Translated (English)</td>
        </tr>
        <tr>
            <td>
                A: 見たりしますね<br>
                B: え、すごい、実写かぁ、えっ、エフェクトつける<br>
                B: ってことはあれだよねー、あのー、編集して、実際の<br>
                B: 動きは人間がやって、<br>
                B: なんかやってみた感<br>
                A: もうなんかこう、光をこう、ラケットとボールが当たる瞬間にこう入れてみたりとか<br>
                A: なんかそのー、ボールがそのー、えー、コートに着地した時に、その着地したところが崩れるエフェクトがあって、なんか穴がコートに開くみたいな<br>
                B: うわっ<br>
                B: そこまでやっちゃうんだ<br>
                A: そうなんですよ<br>
            </td>
            <td>
                A: I do watch it.<br>
                B: Oh, that's cool, it's live-action, huh, with effects.<br>
                B: So that means, um, editing it, the actual<br>
                B: movements are done by humans,<br>
                B: kind of giving it a try.<br>
                A: I just, like, tried adding light, like, at the moment the racket hits the ball,<br>
                A: like, when the ball, um, lands on the court, there's an effect where the landing spot crumbles, like
                a hole opens
                up in the court.<br>
                B: Woah<br>
                B: You go that far.<br>
                A: Yes, that's right.<br>
            </td>
        </tr>
    </table>

    <h4>Generated spoken dialogue</h4>
    <div class="content-container">
        <table border="1" class="inlineTable" id="AudioTable1">
            <col width="300">
            <col width="300">
            <col width="300">
            <col width="300">
            <col width="300">

            <tr>
                <td>Ground Truth</td>
                <td>Resynthesized</td>
                <td>dGSLM</td>
                <td>Baseline</td>
                <td>Proposed</td>
            </tr>
        </table>

    </div>

    <h3>Sample 2</h3>
    <h4>Input written dialogue</h4>
    * (LAU) indicates laughter
    <br><br>

    <table border="1" class="inlineTable" id="TextTable2">
        <col width="750">
        <col width="750">

        <tr>
            <td>Original (Japanese)</td>
            <td>Translated (English)</td>
        </tr>
        <tr>
            <td>
                B: なかなかないよね<br>
                A: ふーん、自分で行く、よね、それこそファーストフード<br>
                A: くらい、よ<br>
                B: (LAU)<br>
                A: お安い、回転寿司の方が落ち着くし<br>
                A: ねー、いっぱい食べれるしね(LAU)、そうなのよ、結局ね、結局そうなんですよ、結局、そうなる、そこに行くんです<br>
                A: やっぱりすごいです<br>
                B: うん、チェーン店は、偉大ということで<br>
                B: はい、一旦これで、おわりでいい?<br>
                A: はい、いいですかね<br>
            </td>
            <td>
                B: It's pretty rare, isn't it?<br>
                A: Hmm, you'd go there yourself, right, especially for fast food.<br>
                A: At least, right.<br>
                B: (LAU)<br>
                A: It's cheaper, and I feel more at ease at conveyor belt sushi places.<br>
                A: Right? You can eat a lot (LAU), exactly, in the end, that's what it comes down to, eventually,
                that's where we go.<br>
                A: It's really amazing.<br>
                B: Yeah, chain stores are, in a sense, remarkable.<br>
                B: Alright, can we conclude this for now?<br>
                A: Yes, is that okay?<br>
            </td>
        </tr>
    </table>


    <h4>Generated spoken dialogue</h4>
    <div class="content-container">
        <table border="1" class="inlineTable" id="AudioTable2">
            <col width="300">
            <col width="300">
            <col width="300">
            <col width="300">
            <col width="300">

            <tr>
                <td>Ground Truth</td>
                <td>Resynthesized</td>
                <td>dGSLM</td>
                <td>Baseline</td>
                <td>Proposed</td>
            </tr>
        </table>
    </div>
    <br>


    <h2>2. LLM samples</h2>
    Using the written dialogues from the test set as prompts, we generated continuations of those dialogues, consisting
    of 6 turns, using GPT-4.
    Then, we split each sentence at punctuation marks ("。" and "！").
    Finally, we generated the corresponding spoken dialogue segments using the proposed system.
    Below, we present two samples of the generated audio, along with the corresponding written dialogues (in Japanese)
    and their automatic translations (in English).


    <h3>Sample 1</h3>
    <table border="1" class="inlineTable" id="MixTable1">
        <col width="500">
        <col width="500">
        <col width="500">

        <tr>
            <td>Original (Japanese)</td>
            <td>Translated (English)</td>
            <td>Generated spoken dialogue (Proposed)</td>
        </tr>
        <tr>
            <td>
                B: あはは、サンタさんのシステムがよくできてるよね。<br>
                B: 子供たちが信じてるうちは来るっていう。<br>
                A: 本当に、信じる心が大事だよね。<br>
                A: 大人になってもその気持ちを持ち続けたい。<br>
                B: そうそう、でも実は大人になると、サンタさんの役割を果たすこともあるんだよね。<br>
                A: うん、それがまた一つの魔法のようなものだよね。<br>
                A: 子供たちの夢を守るための。<br>
                B: 確かに。<br>
                B: 自分が子供の頃、サンタさんからのプレゼントを待ってた気持ちを思い出して、それを次の世代にも伝えたい。<br>
                A: それが一番のクリスマスの魔法だね。<br>
            </td>
            <td>
                B: Haha, the system of Santa is well thought out, isn’t it?<br>
                B: He comes as long as the children believe.<br>
                A: Really, having a believing heart is important.<br>
                A: I want to keep that feeling even as an adult.<br>
                B: Exactly, but actually, when we become adults, sometimes we play the role of Santa.<br>
                A: Yeah, that’s like a kind of magic, isn’t it?<br>
                A: For protecting the dreams of the children.<br>
                B: Definitely.<br>
                B: I remember the feeling of waiting for presents from Santa when I was a child, and I want to pass that
                on to the next generation.<br>
                A: That’s the real magic of Christmas, isn’t it?<br>
            </td>
        </tr>
    </table>

    <h3>Sample 2</h3>
    <table border="1" class="inlineTable" id="MixTable2">
        <col width="500">
        <col width="500">
        <col width="500">

        <tr>
            <td>Original (Japanese)</td>
            <td>Translated (English)</td>
            <td>Generated spoken dialogue (Proposed)</td>
        </tr>
        <tr>
            <td>
                A: お菓子とかお茶とか買って自分へのご褒美してるの？<br>
                B: そうそう、そういう感じ！<br>
                B: 特に気に入ってるスイーツとか、新しい飲み物が出てたら試してみたり。<br>
                A: それいいね。<br>
                A: 自分へのリワードって大切だと思う。<br>
                A: 私も試験とか勉強がんばった時に、好きな映画を家で見たりしてる。<br>
                B: ああ、それもいいね。<br>
                B: やっぱり自分を褒めてあげることって大事。<br>
                B: 自分の努力を認めて、次へのモチベーションにもつながるから。<br>
                A: 本当に。<br>
                A: それに、自分で自分を褒めることで、他人の評価に左右されなくなる気がするよね。<br>
                B: うん、それもそうだね。<br>
                B: 自分の価値を自分で見つけることができれば、心も安定するし、自信も持てるようになる。<br>
            </td>
            <td>
                A: Are you treating yourself with sweets or tea or something?<br>
                B: Yeah, exactly, that kind of feeling!<br>
                B: Especially if there are sweets I like, or if there's a new drink, I like to try it.<br>
                A: That's nice.<br>
                A: I think rewarding oneself is important.<br>
                A: When I work hard on exams or studying, I watch my favorite movies at home.<br>
                B: Oh, that's also nice.<br>
                B: It really is important to praise oneself.<br>
                B: Recognizing one's own effort also connects to motivation for the next task.<br>
                A: Absolutely.<br>
                A: Also, by praising oneself, I feel like we become less affected by other people's evaluations.<br>
                B: Yeah, that's true.<br>
                B: If you can find your own value, you become more stable mentally, and you can also gain
                confidence.<br>
            </td>
        </tr>
    </table>


    <script>
        function handleWaveData(waveData, tableId, createRow) {
            const table = document.getElementById(tableId);
            const wavesurfers = [];

            let row
            if (createRow) {
                row = table.insertRow(-1);
            } else {
                var rowCount = table.rows.length;
                row = table.rows[rowCount - 1];
            }

            waveData.forEach((data, index) => {
                // if (index === 0) {
                //     const row = table.insertRow(-1);
                // }
                // const row = table.rows[1];
                const cell = row.insertCell(-1);
                cell.id = `cell_${data.cell}_header_waveform`;
                const button = document.createElement('button');
                button.id = `cell_${data.cell}_header`;
                button.className = 'play-button-demo btn btn-primary';
                button.innerHTML = '<i class="fa fa-play"></i> Play / <i class="fa fa-pause"></i> Pause';
                button.addEventListener('click', () => wavesurfers[index].playPause());
                cell.appendChild(button);

                const wavesurfer = WaveSurfer.create({
                    container: `#cell_${data.cell}_header_waveform`,
                    waveColor: "skyblue",
                    progressColor: "darkblue",
                    splitChannels: true,
                    responsive: true,
                });

                wavesurfer.load(data.path);
                wavesurfers.push(wavesurfer);
            });
        }

        const waveData1 = [
            { path: './wav/mos/Ground_Truth/00029_00037_00001_00008_15_29.073041.wav', cell: '11' },
            { path: './wav/mos/Resynthesized/00029_00037_00001_00008_15_29.073041.wav', cell: '12' },
            { path: './wav/mos/dGSLM/00029_00037_00001_00008_15_29.073041.wav', cell: '13' },
            { path: './wav/mos/Baseline/00029_00037_00001_00008_15_29.073041.wav', cell: '14' },
            { path: './wav/mos/Proposed/00029_00037_00001_00008_15_29.073041.wav', cell: '15' },
        ];
        handleWaveData(waveData1, 'AudioTable1', true)

        const waveData2 = [
            { path: './wav/mos/Ground_Truth/00031_00040_00001_00010_23_31.675083.wav', cell: '21' },
            { path: './wav/mos/Resynthesized/00031_00040_00001_00010_23_31.675083.wav', cell: '22' },
            { path: './wav/mos/dGSLM/00031_00040_00001_00010_23_31.675083.wav', cell: '23' },
            { path: './wav/mos/Baseline/00031_00040_00001_00010_23_31.675083.wav', cell: '24' },
            { path: './wav/mos/Proposed/00031_00040_00001_00010_23_31.675083.wav', cell: '25' },
        ];
        handleWaveData(waveData2, 'AudioTable2', true)

        const waveData3 = [
            { path: './wav/gpt4/00034_00038_00001_00004_4_26.433208.wav', cell: '31' },
        ];
        handleWaveData(waveData3, 'MixTable1', false)

        const waveData4 = [
            { path: './wav/gpt4/00100_00075_00001_00001_5_27.096375.wav', cell: '41' },
        ];
        handleWaveData(waveData4, 'MixTable2', false)

    </script>

</body>

</html>